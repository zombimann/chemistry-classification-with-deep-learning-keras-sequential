{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "MSE1065-Lab7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9KyPjqRbaME",
        "colab_type": "text"
      },
      "source": [
        "## *Using neural networks to predict and classify crystal structures of elements*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ9-gM7EbaMJ",
        "colab_type": "text"
      },
      "source": [
        "Grade 25 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUCSSNwdbaMK",
        "colab_type": "text"
      },
      "source": [
        "Packages\n",
        "\n",
        "<ul>\n",
        "    <li> Pymatgen </li>\n",
        "    <li> Mendeleev </li>\n",
        "    <li> Keras </li>\n",
        "    <li> Pandas </li>\n",
        "    </ul>\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WOIzcZ-baMM",
        "colab_type": "text"
      },
      "source": [
        "**Why?** Neural networks are widely used for image classification, learning structures and substructures within the data to identify patterns. Such neural network based classifiers can identify patterns and correlations within the data.\n",
        "\n",
        "**What?** In this tutorial we will learn how to use Neural Networks to create a Classification Model to estimate the ground state of the crystal structure. <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UULyfjAfbaMO",
        "colab_type": "text"
      },
      "source": [
        "In Lab-5, we performed logistic regression to predict crystal structure for a number of elements. We are going to perform the same exercise but with neural networks. We are also going to use a powerful neural network library keras to create and fit the neural network. Neural network provided by SKLearn does good job in fitting but is limited in functionality. eg. we can't modify the loss function to include regularizations terms. To avoid overfittin, a very powerful technique known as dropout is used by Neural network community. This functionality is missing in the current implementation of SKLearn. For these reasons, we will switch to keras to train our Neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXF6-qq1baMP",
        "colab_type": "text"
      },
      "source": [
        "### 1. Getting a dataset\n",
        "\n",
        "In this section we will query both [Pymatgen](http://pymatgen.org/) and [Mendeleev](https://mendeleev.readthedocs.io/en/stable/) to get a complete set of properties per element. We will use this data to create the cases from which the model will train and test.\n",
        "<br>\n",
        "<br>\n",
        "In this first snippet of code we will import all relevant libraries, the elements that will be turned into cases and the properties that will serve as the attributes for the cases. We will get 49 entries (which is a small dataset), but should give us a somewhat accurate prediction. We will also include some values to \"patch\" some unknown values in the dataset. It is important to note that more entries would move the prediction closer to the real value, and so would more attributes.\n",
        "<br>\n",
        "<br>\n",
        "The elements listed were chosen because querying them for these properties yields a dataset with few unknown values, and because they represent the three most common crystallographic structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIKoHO7A1UKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install pymatgen\n",
        "# !pip install mendeleev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyeEYJpKbaMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pymatgen as pymat\n",
        "import mendeleev as mendel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WtIWtTabaMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fcc_elements = [\"Ag\", \"Al\", \"Au\", \"Cu\", \"Ir\", \"Ni\", \"Pb\", \"Pd\", \"Pt\", \"Rh\", \"Th\", \"Yb\"]\n",
        "bcc_elements = [\"Ba\", \"Cr\", \"Cs\", \"Eu\", \"Fe\", \"Li\", \"Mn\", \"Mo\", \"Na\", \"Nb\", \"Rb\", \"Ta\", \"V\", \"W\" ]\n",
        "hcp_elements = [\"Be\", \"Ca\", \"Cd\", \"Co\", \"Dy\", \"Er\", \"Gd\", \"Hf\", \"Ho\", \"Lu\", \"Mg\", \"Re\", \n",
        "                \"Ru\", \"Sc\", \"Tb\", \"Ti\", \"Tl\", \"Tm\", \"Y\", \"Zn\", \"Zr\"]\n",
        "#others = [\"Si\", \"Ge\"] # \"Si\" and \"Ge\" are Face-centered diamond-cubic;\n",
        "\n",
        "\n",
        "# This is the list containing all the above elements\n",
        "elements = fcc_elements  + bcc_elements + hcp_elements\n",
        "\n",
        "# Below is the list of features that we need to obtain from Mendeleev database\n",
        "querable_mendeleev = [\"atomic_number\", \"atomic_volume\", \"boiling_point\",\n",
        "                      \"en_ghosh\",  \"evaporation_heat\", \"heat_of_formation\",\n",
        "                     \"lattice_constant\", \"specific_heat\"]\n",
        "\n",
        "# Below is the list of features that we need to obtain from Pymaten database\n",
        "querable_pymatgen = [\"atomic_mass\", \"atomic_radius\", \"electrical_resistivity\",\n",
        "                     \"molar_volume\", \"bulk_modulus\", \"youngs_modulus\",\n",
        "                     \"average_ionic_radius\", \"density_of_solid\",\n",
        "                     \"coefficient_of_linear_thermal_expansion\"]\n",
        "\n",
        "#The list below includes all the properties\n",
        "querable_values = querable_mendeleev + querable_pymatgen"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz3YpaL7baMi",
        "colab_type": "text"
      },
      "source": [
        "After setting these values, we will proceed with our queries. Depending on the database (either Pymatgen or Mendeleev) where the property can be found, the code below fills up a list with the properties of each of the elements. To visualize how the dataset we just created looks, we will use the [Pandas](https://pandas.pydata.org/) library to display it. This library will take the list of lists and show it in a nice, user-friendly table with the properties as the column headers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYchmwDQbaMj",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 1. (2 points)** Fill in the code snippet below to query Pymatgen and Mendeleev for the required properties.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1jZGzMdbaMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_values = [] # Values for Attributes\n",
        "all_labels = [] # Crystal structure (Property to be estimated)\n",
        "\n",
        "for item in elements:\n",
        "    element_values = []\n",
        "    \n",
        "    # This section queries Mendeleev\n",
        "    mend = mendel.element(item)\n",
        "    for query in querable_mendeleev:\n",
        "      element_values.append(eval(\"mend.\" + query))\n",
        "\n",
        "\n",
        "    # This section queries Pymatgen\n",
        "    pyma = pymat.Element(item)\n",
        "    for query in querable_pymatgen:\n",
        "      element_values.append(eval(\"pyma.\" + query))\n",
        "        \n",
        "    all_values.append(element_values) # All lists are appended to another list, creating a list of lists\n"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89XLCC0FbaMs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf5d68fd-cd65-4f27-d5b0-4337532f3980"
      },
      "source": [
        "# Only run this cell once you are done with data query part in the cell above.\n",
        "# This cell is for debugging purpose. \n",
        "assert(len(all_values) == len(elements)),\"Len of all_values not equal to len of elements.\"\n",
        "len(elements)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61BUVlkObaM5",
        "colab_type": "text"
      },
      "source": [
        "### One-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjYCMh8rbaM6",
        "colab_type": "text"
      },
      "source": [
        "Under binary classification, we assign the value of 1 to first class and 0 to the second class. To handle the multi-class classificiation problems, we employ a method known as one-hot encoding. For each sample, we assign a vector instead of a single value. This vector has dimensionality equal to the number of classes in the dataset. All the enteries are zero except the one corresponding to its class. \n",
        "\n",
        "As an example, in this lab we are classifying elements into 'FCC', 'BCC' or 'HCP. So there are 3 classes in this dataset. For each input sample we will assign a output vector or class label of dimensionality 3. An element with 'FCC' crystal can be assigned as [1,0,0]. An element with 'BCC' crystal can have class label as [0,1,0] and 'HCP' will be represented as [0,0,1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRAwerrabaM7",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 2. (2 points)** Convert outY into class labels using one-hot encoding as explained above\n",
        ".</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLfw5Io9baM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad6a7575-1df6-4838-fe2b-35157a7647d5"
      },
      "source": [
        "#Ques \n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "all_labels = []\n",
        "\n",
        "for item in elements:\n",
        "  if item in fcc_elements:\n",
        "    all_labels.append(np.array([1, 0, 0]).astype(int))\n",
        "  elif item in bcc_elements:\n",
        "    all_labels.append(np.array([0, 1, 0]).astype(int))\n",
        "  elif item in hcp_elements:\n",
        "    all_labels.append(np.array([0, 0, 1]).astype(int))\n",
        "    \n",
        "all_labels = np.array(all_labels)\n",
        "all_labels.shape"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1afqPFFzbaNK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "fe714fd0-8418-4a3e-eb4b-528528098009"
      },
      "source": [
        "# This code replaces some missing values in the data. You don't need to fill anything here. Just run this cell before moving forward.\n",
        "# Pandas Dataframe\n",
        "df = pd.DataFrame(all_values, columns=querable_values)\n",
        "\n",
        "# We will patch some of the values that are not available in the datasets.\n",
        "\n",
        "# Value for the CTE of Cesium\n",
        "index_Cs = df.index[df['atomic_number'] == 55]\n",
        "df.iloc[index_Cs, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000097 \n",
        "# Value from: David R. Lide (ed), CRC Handbook of Chemistry and Physics, 84th Edition. CRC Press. Boca Raton, Florida, 2003\n",
        "\n",
        "# Value for the CTE of Rubidium\n",
        "index_Rb = df.index[df['atomic_number'] == 37]\n",
        "df.iloc[index_Rb, df.columns.get_loc(\"coefficient_of_linear_thermal_expansion\")] = 0.000090 \n",
        "# Value from: https://www.azom.com/article.aspx?ArticleID=1834\n",
        "\n",
        "# Value for the Evaporation Heat of Ruthenium\n",
        "index_Ru = df.index[df['atomic_number'] == 44]\n",
        "df.iloc[index_Ru, df.columns.get_loc(\"evaporation_heat\")] = 595 # kJ/mol \n",
        "# Value from: https://www.webelements.com/ruthenium/thermochemistry.html\n",
        "\n",
        "# Value for the Bulk Modulus of Zirconium\n",
        "index_Zr = df.index[df['atomic_number'] == 40]\n",
        "df.iloc[index_Zr, df.columns.get_loc(\"bulk_modulus\")] = 94 # GPa \n",
        "# Value from: https://materialsproject.org/materials/mp-131/\n",
        "# Value for the Bulk Modulus of Germanium\n",
        "index_Ge = df.index[df['atomic_number'] == 32]\n",
        "df.iloc[index_Ge, df.columns.get_loc(\"bulk_modulus\")] = 77.2 # GPa \n",
        "# Value from: https://www.crystran.co.uk/optical-materials/germanium-ge\n",
        "\n",
        "# Value for the Young's Modulus of Germanium\n",
        "index_Ge = df.index[df['atomic_number'] == 32]\n",
        "df.iloc[index_Ge, df.columns.get_loc(\"youngs_modulus\")] = 102.7 # GPa \n",
        "# Value from: https://www.crystran.co.uk/optical-materials/germanium-ge\n",
        "\n",
        "df.head(n=10)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>atomic_number</th>\n",
              "      <th>atomic_volume</th>\n",
              "      <th>boiling_point</th>\n",
              "      <th>en_ghosh</th>\n",
              "      <th>evaporation_heat</th>\n",
              "      <th>heat_of_formation</th>\n",
              "      <th>lattice_constant</th>\n",
              "      <th>specific_heat</th>\n",
              "      <th>atomic_mass</th>\n",
              "      <th>atomic_radius</th>\n",
              "      <th>electrical_resistivity</th>\n",
              "      <th>molar_volume</th>\n",
              "      <th>bulk_modulus</th>\n",
              "      <th>youngs_modulus</th>\n",
              "      <th>average_ionic_radius</th>\n",
              "      <th>density_of_solid</th>\n",
              "      <th>coefficient_of_linear_thermal_expansion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>47</td>\n",
              "      <td>10.30</td>\n",
              "      <td>2485.0</td>\n",
              "      <td>0.147217</td>\n",
              "      <td>254.1</td>\n",
              "      <td>284.9</td>\n",
              "      <td>4.09</td>\n",
              "      <td>0.237</td>\n",
              "      <td>107.868200</td>\n",
              "      <td>1.60</td>\n",
              "      <td>1.630000e-08</td>\n",
              "      <td>10.27</td>\n",
              "      <td>100.0</td>\n",
              "      <td>83.0</td>\n",
              "      <td>1.086667</td>\n",
              "      <td>10490.0</td>\n",
              "      <td>0.000019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13</td>\n",
              "      <td>10.00</td>\n",
              "      <td>2740.0</td>\n",
              "      <td>0.150078</td>\n",
              "      <td>284.1</td>\n",
              "      <td>330.9</td>\n",
              "      <td>4.05</td>\n",
              "      <td>0.900</td>\n",
              "      <td>26.981539</td>\n",
              "      <td>1.25</td>\n",
              "      <td>2.700000e-08</td>\n",
              "      <td>10.00</td>\n",
              "      <td>76.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>0.675000</td>\n",
              "      <td>2700.0</td>\n",
              "      <td>0.000023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>79</td>\n",
              "      <td>10.20</td>\n",
              "      <td>3080.0</td>\n",
              "      <td>0.261370</td>\n",
              "      <td>340.0</td>\n",
              "      <td>368.2</td>\n",
              "      <td>4.08</td>\n",
              "      <td>0.129</td>\n",
              "      <td>196.966569</td>\n",
              "      <td>1.35</td>\n",
              "      <td>2.200000e-08</td>\n",
              "      <td>10.21</td>\n",
              "      <td>220.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>1.070000</td>\n",
              "      <td>19300.0</td>\n",
              "      <td>0.000014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>29</td>\n",
              "      <td>7.10</td>\n",
              "      <td>2840.0</td>\n",
              "      <td>0.151172</td>\n",
              "      <td>304.6</td>\n",
              "      <td>337.4</td>\n",
              "      <td>3.61</td>\n",
              "      <td>0.385</td>\n",
              "      <td>63.546000</td>\n",
              "      <td>1.35</td>\n",
              "      <td>1.720000e-08</td>\n",
              "      <td>7.11</td>\n",
              "      <td>140.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>0.820000</td>\n",
              "      <td>8920.0</td>\n",
              "      <td>0.000017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>77</td>\n",
              "      <td>8.54</td>\n",
              "      <td>4403.0</td>\n",
              "      <td>0.251060</td>\n",
              "      <td>604.0</td>\n",
              "      <td>669.0</td>\n",
              "      <td>3.84</td>\n",
              "      <td>0.133</td>\n",
              "      <td>192.217000</td>\n",
              "      <td>1.35</td>\n",
              "      <td>4.700000e-08</td>\n",
              "      <td>8.52</td>\n",
              "      <td>320.0</td>\n",
              "      <td>528.0</td>\n",
              "      <td>0.765000</td>\n",
              "      <td>22650.0</td>\n",
              "      <td>0.000006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>28</td>\n",
              "      <td>6.60</td>\n",
              "      <td>3005.0</td>\n",
              "      <td>0.147207</td>\n",
              "      <td>378.6</td>\n",
              "      <td>430.1</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.443</td>\n",
              "      <td>58.693400</td>\n",
              "      <td>1.35</td>\n",
              "      <td>7.200000e-08</td>\n",
              "      <td>6.59</td>\n",
              "      <td>180.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>8908.0</td>\n",
              "      <td>0.000013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>82</td>\n",
              "      <td>18.30</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>0.177911</td>\n",
              "      <td>177.8</td>\n",
              "      <td>195.2</td>\n",
              "      <td>4.95</td>\n",
              "      <td>0.159</td>\n",
              "      <td>207.200000</td>\n",
              "      <td>1.80</td>\n",
              "      <td>2.100000e-07</td>\n",
              "      <td>18.26</td>\n",
              "      <td>46.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.122500</td>\n",
              "      <td>11340.0</td>\n",
              "      <td>0.000029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>46</td>\n",
              "      <td>8.90</td>\n",
              "      <td>3413.0</td>\n",
              "      <td>0.144028</td>\n",
              "      <td>372.4</td>\n",
              "      <td>376.6</td>\n",
              "      <td>3.89</td>\n",
              "      <td>0.244</td>\n",
              "      <td>106.420000</td>\n",
              "      <td>1.40</td>\n",
              "      <td>1.080000e-07</td>\n",
              "      <td>8.56</td>\n",
              "      <td>180.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>0.846250</td>\n",
              "      <td>12023.0</td>\n",
              "      <td>0.000012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>78</td>\n",
              "      <td>9.10</td>\n",
              "      <td>4100.0</td>\n",
              "      <td>0.256910</td>\n",
              "      <td>470.0</td>\n",
              "      <td>565.7</td>\n",
              "      <td>3.92</td>\n",
              "      <td>0.133</td>\n",
              "      <td>195.084000</td>\n",
              "      <td>1.35</td>\n",
              "      <td>1.060000e-07</td>\n",
              "      <td>9.09</td>\n",
              "      <td>230.0</td>\n",
              "      <td>168.0</td>\n",
              "      <td>0.805000</td>\n",
              "      <td>21090.0</td>\n",
              "      <td>0.000009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>45</td>\n",
              "      <td>8.30</td>\n",
              "      <td>4000.0</td>\n",
              "      <td>0.140838</td>\n",
              "      <td>494.0</td>\n",
              "      <td>556.0</td>\n",
              "      <td>3.80</td>\n",
              "      <td>0.244</td>\n",
              "      <td>102.905500</td>\n",
              "      <td>1.35</td>\n",
              "      <td>4.300000e-08</td>\n",
              "      <td>8.28</td>\n",
              "      <td>380.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.745000</td>\n",
              "      <td>12450.0</td>\n",
              "      <td>0.000008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   atomic_number  ...  coefficient_of_linear_thermal_expansion\n",
              "0             47  ...                                 0.000019\n",
              "1             13  ...                                 0.000023\n",
              "2             79  ...                                 0.000014\n",
              "3             29  ...                                 0.000017\n",
              "4             77  ...                                 0.000006\n",
              "5             28  ...                                 0.000013\n",
              "6             82  ...                                 0.000029\n",
              "7             46  ...                                 0.000012\n",
              "8             78  ...                                 0.000009\n",
              "9             45  ...                                 0.000008\n",
              "\n",
              "[10 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbm6m4SebaNS",
        "colab_type": "text"
      },
      "source": [
        "### 2. Processing and Organizing Data\n",
        "\n",
        "We again normalize the data and organize it into training and testing sets as before.\n",
        "\n",
        "##### SETS\n",
        "\n",
        "We have 47 elements for which the crystal structure is known and we will use 40 of these as a training set and the remaining 7 as testing set.\n",
        "\n",
        "##### NORMALIZATION\n",
        "\n",
        "We will again use the Standard Score Normalization, which subtracts the mean of the feature and divide by its standard deviation.\n",
        "\n",
        "<span style=\"font-size:2em;\">$ \\frac{X - µ}{σ} $ </span>\n",
        "\n",
        "While our model might converge without feature normalization, the resultant model would be difficult to train and would be dependent on the choice of units used in the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUzVIEuTbaNT",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 4. (3 points)** Perform normalization on the input features stored in the dataframe df </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksey5XHjbaNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normX = (df-df.mean())/df.std()\n"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kckDz8QkbaNa",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 5. (1 point)** Check if there is any Nan in the dataframe normX </font> "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_yo_BNbbaNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fa6e57d-660a-42fe-b1bf-8bb3672d245e"
      },
      "source": [
        "normX.isnull().values.any()"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3bf8gNBbaNk",
        "colab_type": "text"
      },
      "source": [
        "##### Split data into training and test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6GVUArEbaNm",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 6. (1 point)** Use SKLearn's train_test_split to obtain training and test set. Use 20% as test set. Output are stored in all_labels variable. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mefcUt4QbaOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "all_labels = train_test_split(all_labels, test_size=0.2, random_state=42)\n",
        "vals = train_test_split(normX.values, test_size=0.2, random_state=42)\n",
        "df_train = vals[0]\n",
        "df_test = vals[1]\n"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK0eMjOWbaOO",
        "colab_type": "text"
      },
      "source": [
        "### 3. Creating the Model\n",
        "\n",
        "For this classification, we will use a simple sequential neural network with one densely connected hidden layer. The optimizer used will be [RMSPropOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer) (Root Mean Square Propagation).\n",
        "\n",
        "To learn more about Root Mean Squared Propagation, click [here](https://climin.readthedocs.io/en/latest/rmsprop.html).\n",
        "\n",
        "\n",
        "\n",
        "The key difference between the regression model and the classification model is our metric to measure network performance. While we used mean squared error (between the true outputs and the network's predicted output) for the regression task, we use categorical crossentropy (click [here](https://towardsdatascience.com/demystifying-cross-entropy-e80e3ad54a8) to learn more about it), using classification accuracy as a metric where higher accuracy implies a better network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfNKBwtnbaOP",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue>  In this section we will learn about a powerful python library, keras. This library is easy to use and can fit different neural networks. There are number of tutorials online to help you start with Keras library and we highly recommend to use these sources to finish this lab. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjLWOVeWbaOR",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 7. (1 point)** Import keras library and define a sequential model.  </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcJ12gZ6baOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2qRm0hqbaOY",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 8. (1 point)** Add a dense layer to your model with 10 hidden units. The activation function is 'relu'.  </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn3VfXURbaOZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Dense(10, activation='relu'))"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY3VgvrNbaOi",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 9. (1 point)** Compile your model with 'categorical_crossentropy' loss, 'RMSProp' as optimizer and 'accuracy' as metrics. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1oEJwf0baOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='RMSProp', loss='categorical_crossentropy')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgEu3M1lbaOs",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 10. (1 point)** Fit your model on training set </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eEI6W4uddyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "283ba79f-4e81-4b73-c2f3-25d1c407fa29"
      },
      "source": [
        "df_train.shape"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FquBIBz_baOt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "fc9048bb-8424-4159-a365-592edc8551ed"
      },
      "source": [
        "# df_train = all_values[0]\n",
        "# df_test = all_values[1]\n",
        "\n",
        "\n",
        "model.fit(df_train, np.array(all_labels[0]).reshape(37, 3), batch_size=12, epochs=10)\n",
        "\n",
        "\n"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-a4c479e7314f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m37\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgO0hjG6baO8",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 11. (1 point)** Evaluate your model on training set. The output of your neural network will be a column vector corresponding to one-hot encoding you performed earlier. Choose the column corresponding to maximum value and assign it crystal structure accordingly </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW20dqtbbaO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Enter your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNPMlqxAbaPH",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 12. (1 point)** Evaluate your code on test set </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi8AiqbYbaPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Enter your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhN7IQ3_baPN",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue>  Congrats you have trained and evaluted your first model using Keras library. Lets get more practice with this library.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnC9Ufm_baPO",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 13. (5 points)** Increase the number of hidden units in your model to 50. Compile and fit this new neural network on training set. Evaluate the performance on training set as well as test set. Please note that you are fitting the model only on training set and \"seperatly\" evaluating the performance on training and test set  </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCTx-lR2baPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Enter your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KFXdxULbaPT",
        "colab_type": "text"
      },
      "source": [
        "* <font color=blue> **Exercise 14. (5 points)** Add another dense layer to your model. Keep the number of hidden units in both layers to be 25. Compile and fit this new neural network on training set. Evaluate the performance on training set as well as test set. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUCWi-rxbaPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Enter your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFkP24LHbaPa",
        "colab_type": "text"
      },
      "source": [
        "### Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G0P3fowbaPb",
        "colab_type": "text"
      },
      "source": [
        "<ul>\n",
        "    <li> Try different parameters of the keras model and evaluate the performance on this dataset. Eg. you may want to change the number of epochs, optimization algorithm, batch size etc.</li>\n",
        "    <li> Neural network training is inherently random in nature. Eg. given the initial values of the weights, models might behave differently. As we are working with a very small dataset, that difference can be small. Nevertheless, I highly recommend to read about randomness in neural network training, work out different random settings and evaluate your models. Another randomness in your results can be due to different training and test splits </li>\n",
        "    <li> You can also use cross-validation to select different neural network architecture. </li>\n",
        "    <li> Dropout is a method to obtain sparse neural networks. Read about this method and implement a dropout network on this data set. </li>\n",
        "    </ul>\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia0JEdRXbaQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}